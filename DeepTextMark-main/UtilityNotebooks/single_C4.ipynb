{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892434b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet_ic')\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import nltk.corpus\n",
    "from sematch.semantic.similarity import WordNetSimilarity\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import inflect\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c411756",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('databricks_dolly_15k.jsonl') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fcf530",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for i in data:\n",
    "    sentences.append(i['instruction'])\n",
    "    sentences.append(i['context'])\n",
    "    sentences.append(i['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51640dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    # Use regular expression to remove special characters except single and double quotations\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\']', '', text)  # Allow single and double quotations\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    return text\n",
    "\n",
    "sentences = [remove_special_characters(sentence) for sentence in sentences]\n",
    "sentences = [sentence for sentence in sentences if sentence.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_words(sentence):\n",
    "    return sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercases all words currently for simplicity.\n",
    "def watermark_sentence(orig_sentence):\n",
    "    # Can add\n",
    "    sentence = [word.lower() for word in orig_sentence.copy()]\n",
    "    \n",
    "    # Get the best synonym starting with the word at the end of the sentence.\n",
    "    result = get_best_synonym(sentence)\n",
    "    \n",
    "    if result is None:\n",
    "        return None\n",
    "    \n",
    "    best_synonym, best_synonym_index = result\n",
    "    \n",
    "    # Replace the target word with the synonym.\n",
    "    sentence[best_synonym_index] = best_synonym\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_plural_or_singular(word):\n",
    "    # Initialize the inflect engine\n",
    "    p = inflect.engine()\n",
    "    \n",
    "    # Get the singular form of the word\n",
    "    singular_form = p.singular_noun(word)\n",
    "    \n",
    "    # Compare the singular form with the original word\n",
    "    if singular_form == False:\n",
    "        return \"Singular\"\n",
    "    else:\n",
    "        return \"Plural\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def is_stopword(word):\n",
    "    return word.lower() in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eab853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_synonyms(word):\n",
    "    word = word.lower()\n",
    "    synonyms = []\n",
    "    scores = []\n",
    "    for ss in wn.synsets(word):\n",
    "        synonyms.extend([lemma.lower() for lemma in ss.lemma_names()])\n",
    "        for sim in ss.similar_tos():\n",
    "            synonyms_batch = sim.lemma_names()\n",
    "            synonyms.extend(synonyms_batch)\n",
    "    synonyms = set(synonyms)\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    synonyms = [synonym.replace('_',' ') for synonym in synonyms]\n",
    "    \n",
    "    for s in synonyms:\n",
    "        if determine_plural_or_singular(s) != determine_plural_or_singular(word):\n",
    "            synonyms.remove(s)\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f259c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the index and synonym that is the highest scored synonym.\n",
    "def get_best_synonym(sentence):\n",
    "    \n",
    "    score_list = []\n",
    "    word_list = []\n",
    "    \n",
    "    #print(len(sentence))\n",
    "    #print(sentence)\n",
    "    \n",
    "    for i in range(len(sentence)):\n",
    "        word = sentence[i]\n",
    "        if is_stopword(word):\n",
    "            word_list.append(\"none\")\n",
    "            score_list.append(0)\n",
    "        else:\n",
    "            all_synonyms = get_all_synonyms(word)\n",
    "\n",
    "            # Ignore current word if there are no synonyms.\n",
    "            if len(all_synonyms) == 0: \n",
    "                # append 0 score.\n",
    "                score_list.append(0)\n",
    "                word_list.append(\"none\")\n",
    "                continue\n",
    "\n",
    "            wns = WordNetSimilarity()\n",
    "            similarity_scores = [wns.word_similarity(word, syn) for syn in all_synonyms]\n",
    "\n",
    "            # Uncomment the following to see process.\n",
    "            #print(word)\n",
    "            #print(all_synonyms)\n",
    "            #print(similarity_scores)\n",
    "\n",
    "            max_score = max(similarity_scores)\n",
    "            best_synonym_for_current_idx = all_synonyms[similarity_scores.index(max_score)]\n",
    "            \n",
    "            #print(max_score)\n",
    "            #print(best_synonym_for_current_idx)\n",
    "\n",
    "            word_list.append(best_synonym_for_current_idx)\n",
    "            score_list.append(max_score)\n",
    "        \n",
    "    #print(word_list)\n",
    "    #print(score_list)\n",
    "    #best_score_overall = max(score_list)\n",
    "    best_word = []\n",
    "    \n",
    "    best_score_overall = [index for index, value in enumerate(score_list) if value == 1]\n",
    "    \n",
    "    if best_score_overall == 0:\n",
    "        print(\"No available synonyms\")\n",
    "        return None\n",
    "    \n",
    "    #best_word_idx = score_list.index(best_score_overall)\n",
    "    for i in best_score_overall:\n",
    "        best_word.append(word_list[i])\n",
    "    \n",
    "    return best_word, best_score_overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_data_folder = 'c4_llm_multiple_marked_clean/'\n",
    "unmarked_data_folder = 'c4_llm_multiple_unmarked_clean/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee9a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_num_hundreds = 30 #45\n",
    "num_hundreds = 10 #573\n",
    "for i in range(start_num_hundreds + 1, num_hundreds + start_num_hundreds + 1):\n",
    "    start_range = (i - 1) * 100\n",
    "    end_range = (i) * 100\n",
    "    \n",
    "    print(start_range, end_range)\n",
    "    \n",
    "    current_sentences = sentences_token[start_range:end_range]\n",
    "    #print(current_sentences[0])\n",
    "\n",
    "    watermarked_sents = []\n",
    "    unmarked_sents = []\n",
    "    \n",
    "    for i in range(len(current_sentences)):\n",
    "        sentence = current_sentences[i].copy()\n",
    "        \n",
    "        if len(sentence) < 5:\n",
    "            continue\n",
    "        \n",
    "        result = watermark_sentence(sentence)\n",
    "        \n",
    "        if result is not None:\n",
    "            watermarked_sents.append(result)\n",
    "            unmarked_sents.append([word.lower() for word in current_sentences[i]])\n",
    "            \n",
    "    with open(unmarked_data_folder + 'unmarked' + str(start_range) + 'To' + str(end_range) + '.pkl', 'wb') as file:\n",
    "        pickle.dump(unmarked_sents, file)\n",
    "        \n",
    "    with open(marked_data_folder + 'watermarked' + str(start_range) + 'To' + str(end_range) + '.pkl', 'wb') as file:\n",
    "        pickle.dump(watermarked_sents, file)\n",
    "        \n",
    "    print('Finish')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
